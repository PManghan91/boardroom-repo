name: AI Operations Testing Suite

on:
  push:
    branches: [ main, develop, task-* ]
    paths:
      - 'app/core/langgraph/**'
      - 'app/services/ai_state_manager.py'
      - 'app/api/v1/ai_operations.py'
      - 'app/schemas/ai_operations.py'
      - 'tests/**'
      - '.github/workflows/ai-testing.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'app/core/langgraph/**'
      - 'app/services/ai_state_manager.py'
      - 'app/api/v1/ai_operations.py'
      - 'app/schemas/ai_operations.py'
      - 'tests/**'
      - '.github/workflows/ai-testing.yml'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - performance
          - full

env:
  PYTHON_VERSION: '3.11'
  UV_VERSION: '0.1.18'

jobs:
  # Quality gates and setup
  quality-gates:
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.changes.outputs.ai-related }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for AI-related changes
        uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            ai-related:
              - 'app/core/langgraph/**'
              - 'app/services/ai_state_manager.py'
              - 'app/api/v1/ai_operations.py'
              - 'app/schemas/ai_operations.py'
              - 'tests/**'

      - name: Set up Python
        if: steps.changes.outputs.ai-related == 'true'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        if: steps.changes.outputs.ai-related == 'true'
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install dependencies
        if: steps.changes.outputs.ai-related == 'true'
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e ".[test,dev]"

      - name: Lint AI modules
        if: steps.changes.outputs.ai-related == 'true'
        run: |
          source .venv/bin/activate
          ruff check app/core/langgraph/ app/services/ai_state_manager.py app/api/v1/ai_operations.py
          ruff format --check app/core/langgraph/ app/services/ai_state_manager.py app/api/v1/ai_operations.py

      - name: Type check AI modules
        if: steps.changes.outputs.ai-related == 'true'
        run: |
          source .venv/bin/activate
          mypy app/core/langgraph/ app/services/ai_state_manager.py app/api/v1/ai_operations.py

  # Unit tests for AI components
  unit-tests:
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.should-run-tests == 'true'
    strategy:
      matrix:
        test-group:
          - langgraph
          - ai-state-manager
          - meeting-tools
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e ".[test,dev]"

      - name: Run unit tests - LangGraph
        if: matrix.test-group == 'langgraph'
        run: |
          source .venv/bin/activate
          pytest tests/unit/test_langgraph_graph.py -v \
            --cov=app.core.langgraph.graph \
            --cov-report=xml:coverage-langgraph.xml \
            --cov-report=term \
            --junit-xml=test-results-langgraph.xml

      - name: Run unit tests - AI State Manager
        if: matrix.test-group == 'ai-state-manager'
        run: |
          source .venv/bin/activate
          pytest tests/unit/test_ai_state_manager.py -v \
            --cov=app.services.ai_state_manager \
            --cov-report=xml:coverage-ai-state.xml \
            --cov-report=term \
            --junit-xml=test-results-ai-state.xml

      - name: Run unit tests - Meeting Tools
        if: matrix.test-group == 'meeting-tools'
        run: |
          source .venv/bin/activate
          pytest tests/unit/test_meeting_management_tools.py -v \
            --cov=app.core.langgraph.tools.meeting_management \
            --cov-report=xml:coverage-meeting-tools.xml \
            --cov-report=term \
            --junit-xml=test-results-meeting-tools.xml

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-${{ matrix.test-group }}
          path: |
            test-results-*.xml
            coverage-*.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: coverage-*.xml
          flags: unit-tests,${{ matrix.test-group }}
          name: unit-${{ matrix.test-group }}

  # Integration tests for AI operations
  integration-tests:
    runs-on: ubuntu-latest
    needs: [quality-gates, unit-tests]
    if: needs.quality-gates.outputs.should-run-tests == 'true'
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e ".[test,dev]"

      - name: Set up test environment
        run: |
          cp .env.example .env
          echo "DATABASE_URL=postgresql://testuser:testpass@localhost:5432/testdb" >> .env
          echo "REDIS_URL=redis://localhost:6379" >> .env
          echo "ENVIRONMENT=test" >> .env

      - name: Run database migrations
        run: |
          source .venv/bin/activate
          alembic upgrade head

      - name: Run integration tests
        run: |
          source .venv/bin/activate
          pytest tests/integration/test_ai_operations_endpoints.py -v \
            --cov=app.api.v1.ai_operations \
            --cov-report=xml:coverage-integration.xml \
            --cov-report=term \
            --junit-xml=test-results-integration.xml \
            -m "integration and not slow"

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            test-results-integration.xml
            coverage-integration.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: coverage-integration.xml
          flags: integration-tests
          name: integration-ai-operations

  # Performance tests (conditional)
  performance-tests:
    runs-on: ubuntu-latest
    needs: [quality-gates, unit-tests]
    if: |
      needs.quality-gates.outputs.should-run-tests == 'true' && 
      (github.event_name == 'schedule' || 
       github.event_name == 'workflow_dispatch' ||
       contains(github.event.head_commit.message, '[perf-test]'))
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e ".[test,dev]"

      - name: Run performance tests
        run: |
          source .venv/bin/activate
          pytest tests/performance/test_ai_operations_performance.py -v \
            --junit-xml=test-results-performance.xml \
            -m "performance and not slow" \
            --maxfail=3

      - name: Run load tests (if full test mode)
        if: github.event.inputs.test_level == 'full'
        run: |
          source .venv/bin/activate
          pytest tests/performance/test_ai_operations_performance.py -v \
            --junit-xml=test-results-load.xml \
            -m "performance and slow" \
            --maxfail=1

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            test-results-performance.xml
            test-results-load.xml

  # Security and safety tests
  security-tests:
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.should-run-tests == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV and security tools
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
          pip install bandit safety

      - name: Install dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e ".[test,dev]"

      - name: Run security scan with Bandit
        run: |
          bandit -r app/core/langgraph/ app/services/ai_state_manager.py app/api/v1/ai_operations.py \
            -f json -o bandit-report.json || true

      - name: Run dependency security check
        run: |
          safety check --json --output safety-report.json || true

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json

  # Coverage aggregation and reporting
  coverage-report:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always() && needs.quality-gates.outputs.should-run-tests == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install coverage tools
        run: |
          pip install coverage[toml] codecov

      - name: Combine coverage reports
        run: |
          coverage combine unit-test-results-*/coverage-*.xml integration-test-results/coverage-*.xml || true
          coverage report --show-missing
          coverage html -d coverage-html

      - name: Upload combined coverage
        uses: actions/upload-artifact@v3
        with:
          name: combined-coverage-report
          path: coverage-html/

      - name: Coverage status check
        run: |
          coverage report --fail-under=80

  # Test summary and notifications
  test-summary:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, security-tests, coverage-report]
    if: always() && needs.quality-gates.outputs.should-run-tests == 'true'
    
    steps:
      - name: Download test results
        uses: actions/download-artifact@v3

      - name: Generate test summary
        run: |
          echo "# AI Operations Testing Summary" > test-summary.md
          echo "" >> test-summary.md
          echo "## Test Results" >> test-summary.md
          
          # Unit tests summary
          if [ -d "unit-test-results-langgraph" ]; then
            echo "- ✅ LangGraph unit tests: Completed" >> test-summary.md
          else
            echo "- ❌ LangGraph unit tests: Failed" >> test-summary.md
          fi
          
          if [ -d "unit-test-results-ai-state-manager" ]; then
            echo "- ✅ AI State Manager unit tests: Completed" >> test-summary.md
          else
            echo "- ❌ AI State Manager unit tests: Failed" >> test-summary.md
          fi
          
          if [ -d "unit-test-results-meeting-tools" ]; then
            echo "- ✅ Meeting Tools unit tests: Completed" >> test-summary.md
          else
            echo "- ❌ Meeting Tools unit tests: Failed" >> test-summary.md
          fi
          
          # Integration tests summary
          if [ -d "integration-test-results" ]; then
            echo "- ✅ Integration tests: Completed" >> test-summary.md
          else
            echo "- ❌ Integration tests: Failed" >> test-summary.md
          fi
          
          # Performance tests summary
          if [ -d "performance-test-results" ]; then
            echo "- ✅ Performance tests: Completed" >> test-summary.md
          else
            echo "- ⚠️ Performance tests: Skipped or Failed" >> test-summary.md
          fi
          
          # Security tests summary
          if [ -d "security-reports" ]; then
            echo "- ✅ Security tests: Completed" >> test-summary.md
          else
            echo "- ❌ Security tests: Failed" >> test-summary.md
          fi
          
          echo "" >> test-summary.md
          echo "## Coverage Report" >> test-summary.md
          if [ -d "combined-coverage-report" ]; then
            echo "- ✅ Coverage report generated" >> test-summary.md
          else
            echo "- ❌ Coverage report failed" >> test-summary.md
          fi
          
          echo "" >> test-summary.md
          echo "Workflow run: ${{ github.run_id }}" >> test-summary.md
          echo "Commit: ${{ github.sha }}" >> test-summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v3
        with:
          name: test-summary
          path: test-summary.md

      - name: Set workflow status
        run: |
          if [ "${{ needs.unit-tests.result }}" = "failure" ] || 
             [ "${{ needs.integration-tests.result }}" = "failure" ] || 
             [ "${{ needs.coverage-report.result }}" = "failure" ]; then
            echo "Critical tests failed"
            exit 1
          fi